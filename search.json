[
  {
    "objectID": "scripts/blog7.html",
    "href": "scripts/blog7.html",
    "title": "Blog Post 7",
    "section": "",
    "text": "library(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.3\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'readr' was built under R version 4.3.3\n\n\nWarning: package 'purrr' was built under R version 4.3.3\n\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\nWarning: package 'stringr' was built under R version 4.3.3\n\n\nWarning: package 'forcats' was built under R version 4.3.3\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(shiny)\n\nWarning: package 'shiny' was built under R version 4.3.3\n\nlibrary(leaflet)\n\nWarning: package 'leaflet' was built under R version 4.3.3\n\nhousing_cleaned &lt;- readRDS(\"C:/Users/24725/Documents/ma4615-sp25-final-project-team-3/housing_cleaned.rds\") %&gt;%\n  mutate(RACE = factor(RACE, levels = 1:9, labels = c(\n    \"White\",\n    \"Black/African American\",\n    \"American Indian or Alaska Native\",\n    \"Chinese\",\n    \"Japanese\",\n    \"Other Asian or Pacific Islander\",\n    \"Other race, nec\",\n    \"Two major races\",\n    \"Three or more major races\"\n  )))\n\n\nselectInput(\"selected_race\", \"Choose a race:\", choices = sort(unique(housing_cleaned$RACE)))\n\n\nChoose a race:\n\nWhite\nBlack/African American\nAmerican Indian or Alaska Native\nChinese\nJapanese\nOther Asian or Pacific Islander\nOther race, nec\nTwo major races\nThree or more major races\n\n\n\n\n\n\nrenderPlot({\n  housing_cleaned %&gt;%\n    filter(RACE == input$selected_race) %&gt;%\n    ggplot(aes(x = FTOTINC, y = RACE)) +\n    geom_boxplot(fill = \"skyblue\", outlier.shape = NA) +\n    scale_x_log10() +\n    labs(title = paste(\"Income Distribution for Race:\", input$selected_race),\n         x = \"Total Household Income (log scale)\",\n         y = \"Race\")\n})"
  },
  {
    "objectID": "posts/2025-04-14-blog-post-6/blog-post-6.html",
    "href": "posts/2025-04-14-blog-post-6/blog-post-6.html",
    "title": "Blog Post 6",
    "section": "",
    "text": "We campared the relationships between Area (sqft) of houses and price, they have strong positive linear relations, larger house is more expensive. We can also observed that most houses is smaller than 12000 sqrt, and less than one million. From the data perspective, there are significantly more cheaper and smaller houses.  From the chart ‘Price by Furnishing Status’, it’s clear that furnished houses are significantly more expensive, semi-furnished ones are moderate but have a lower bound similar to furnished houses, while unfurnished houses are noticeably cheaper. Perhaps semi-furnished could be a good choice.  From the chart ‘Price by Number of Bedrooms’, it’s clear that houses with one bedroom have a huge price gap compared to others. As the number of bedrooms increases (from 2 to 5), the price of the house increases as well. The price difference between houses with 3, 4, and 5 bedrooms isn’t very large, but for houses with 6 bedrooms, the price decreases, and it’s almost the same as the average price for 3-bedroom houses. This could be due to insufficient sample size, or it might suggest that houses with 6 bedrooms are not as practical compared to those with 4 or 5 bedrooms. We hypothesize that a 5-bedroom house with good furnishings is worth more than a 6-bedroom house.  From the ‘Price vs Parking Space’ chart, it’s evident that houses with 0, 1, and 2 parking spaces show a gradual increase in price, while houses with 3 parking spaces experience a decline. We hypothesize that, similar to the number of bedrooms chart, the reasons could be insufficient sample size or a lack of practicality  This graph compares the correlations between key variables, with darker colors representing stronger correlations. We can see that area and the number of bathrooms have the greatest influence on price, which is quite interesting—bedrooms, in contrast, have a weaker impact than bathrooms. This makes sense: luxury homes often have more bathrooms on each floor, sometimes even more than the number of bedrooms, while smaller houses typically have a one-to-one ratio between bathrooms and bedrooms. The relationship between bedrooms and bathrooms is also not particularly strong. The number of stories has the next highest impact on house price after area and bathrooms. The weak correlation between area and bedrooms suggests that luxury homes may include a wide range of spaces, and bedrooms are not densely packed."
  },
  {
    "objectID": "posts/2025-03-31-blog-post-4/blog-post-4.html",
    "href": "posts/2025-03-31-blog-post-4/blog-post-4.html",
    "title": "Blog Post 4",
    "section": "",
    "text": "One of the most obvious trends in our data was the strong right-skew in housing prices, which we initially observed in Blog Post 3. To go deeper, we explored whether this trend was uniform across locations by analyzing median prices by zip code. We found large geographic variation, with some zip codes having dramatically higher median prices than others. This suggests that the overall skew may actually be masking meaningful disparities in affordability and land value between areas—making the trend more interesting than initially assumed. It also raised the possibility that structural factors, like historic zoning or disinvestment, could be influencing housing markets. We plan to explore further relationships between variables to explain the aforementioned impact of structural factors, uncovering patterns that go beyond simple price distributions. For example, we’re interested in how property size, location (urban vs. rural), and racial demographics may interact with housing prices. Understanding how these variables fit together will help us build models that reflect the intersectionality of housing access, affordability, and equity. By layering in more dimensions of the data, we can move toward a more nuanced analysis of housing inequality.  To get a clearer picture of the typical housing market and prepare for future modeling, we log-transformed the price variable to reduce the influence of extreme values. This normalized the distribution and revealed that most homes fall within a tighter price band than the raw data suggests. This step not only deepens our understanding of the price distribution but also points to potential predictor variables—such as location or neighborhood characteristics—that we can explore more fully through modeling in future posts. Graph 1 describes the distribution of house records amongst all racial groups. It is heavily skewed with the white racial group showing majority in records.  Graph 2 describes the same as the chart above but shows the difference in range of household values between each racial group. Graph 3 describes the distribution between racial groups and the value of their households depending on the market. It is split up into three categories (low, medium and high).  This study aims to analyze housing conditions among different racial groups, focusing on rent prices, geographic location, and homeownership status. By examining these factors, we seek to understand potential disparities in housing affordability and accessibility. While the original analysis did not include housing size, we incorporate square footage as an additional variable to better assess its impact on rent prices and overall housing conditions. To achieve this, we define several key response variables. First, rent price (continuous variable) serves as a primary indicator of housing affordability. Second, homeownership status (binary variable: Yes/No) allows us to explore differences in property ownership rates across racial groups. Additionally, housing quality (either continuous or categorical) provides insight into the standard of living, considering factors such as building conditions and available amenities. These variables are analyzed in relation to explanatory factors such as race, location, household income, education level, and market conditions. For statistical modeling, we employ linear regression to analyze rent prices and housing quality when treated as a continuous variable. Logistic regression is applied to predict homeownership probability based on demographic and economic factors. If housing quality is categorized, we use multinomial logistic regression to capture differences across multiple quality levels. By integrating these models, we aim to provide a comprehensive understanding of the racial disparities in housing affordability and quality."
  },
  {
    "objectID": "posts/2025-03-17-blog-post-2/blog-post-2.html",
    "href": "posts/2025-03-17-blog-post-2/blog-post-2.html",
    "title": "Blog Post 2",
    "section": "",
    "text": "The data comes from the IPUMS website, an individual-level population database containing information for U.S. Census Data for Social, Economic, and Health Research. This dataset can be used to examine the racial disparities in land ownership. The importance of distinguishing between urban and rural residential properties provides insight for analyzing ownership patterns, housing distribution, and land usage across different demographic groups. Historically, the American segregational economic policy has confined minority groups into urban areas, limiting their ability to acquire larger sets of land. Since land and homeownership are major contributors to generational wealth, the dataset can also explore how land distribution contributes to the racial wealth gap.\nThe process of finding this data was not very smooth, the data we downloaded through the website was very large, because it was very many valid variables, when processing the data even though unnecessary data had been removed, the data size was still 78.9mb, which was larger than 50mb, so the dataset could only be reduced. The current sample population is 3405811 rows, the table up to a million rows, sample does exist bias, such as the variable value, many 99999999, which is obviously due to some kind of statistical limitations, the highest is only 99999999, if you really follow this data to statistics, will certainly make the real value is small.\nThe IPUMS USA dataset is widely used in research and policymaking, providing microdata from U.S. censuses and the American Community Survey. Researchers have used it to study migration, labor markets, education, and housing trends, leveraging its longitudinal depth to track societal changes over time. Policymakers rely on IPUMS data to guide urban planning, zoning laws, economic policies, and social programs. For example, housing lot size trends inform infrastructure and land-use decisions. Past research has explored questions like how immigration patterns impact the economy, what drives educational disparities, and how employment trends respond to economic cycles. By offering rich demographic insights, the dataset continues to shape both academic studies and policy development."
  },
  {
    "objectID": "posts/2024-10-04-general-tips/general-tips.html",
    "href": "posts/2024-10-04-general-tips/general-tips.html",
    "title": "General Tips",
    "section": "",
    "text": "Use the tidyverse!\nYou don’t have to tell me what kind of chart something is. For example, the below is not a useful start to a sentence.\n\n\nThe graph presents a horizontal bar chart …\n\n\nEach page should be largely standalone.\nSometimes small tables or even inline numbers are better than a figure.\nRedundant colors (e.g. bar charts where each bar is a different color that doesn’t signify anything) often don’t help.\nProvide some details on how much data was removed in your cleaning process.\nUse the tidyverse!\nImagine I’m an impatient boss. Show me only what is important and relevant.\nCleaning must be entirely in R\nDon’t say things like, well if only everyone did like so and so than everything would be better. There are many things hiding behind the data that would go to explain things. This is an example of a bad conclusion.\n\n\nThe world could benefit form modeling its education systems after Europe’s.\n\nIt is fine to talk about how the European system is better according to certain metrics, but don’t assume that can easily translate to other regions.\n\nDon’t talk about your “journey”. The blog posts tell the story of your journey. The main pages should focus on the data and your findings.\n\n\nUse the tidyverse!\nNo but seriously, when asking ChatGPT to do your project for you, make sure to tell it to use the tidyverse, not base R."
  },
  {
    "objectID": "posts/2023-10-15-getting-started/getting-started.html",
    "href": "posts/2023-10-15-getting-started/getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "Below, the items marked with [[OP]] should only be done by one person on the team.\n\nTo get started\n\n[[OP]] One person from the team should click the Github Classroom link on Teams.\n[[OP]] That person types in the group name for their group.\nThe rest of the team now clicks the Github Classroom link and selects their team from the dropdown list.\nFinally, each of you can clone the repository to your laptop like a normal assignment.\n\n\n\nSetting up the site\n\n[[OP]] Open the terminal and run quarto publish gh-pages.\n[[OP]] Select Yes to the prompt:  ? Publish site to https://sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME/ using gh-pages? (Y/n)\n[[OP]] Wait for the process to finish.\nOnce it is done, you can go to the URL it asked you about to see your site.\n\nNote: This is the process you will use every time you want to update your published site. Make sure to always follow the steps below for rendering, previewing, and committing your changes before doing these publish steps. Anyone can publish in the future.\n\n\nCustomize your site\n\n[[OP]] Open the _quarto.yml file and update the title to include your team name.\n[[OP]] Go to the about.qmd and remove the TF’s and professor’s names.\nadd your own along with a short introduction and a link to your Github user page.\n[[OP]] Render the site.\n[[OP]] Check and make sure you didn’t get any errors.\n[[OP]] Commit your changes and push.\n[[OP]] Repeat the steps under Setting up your site.\n\nOnce one person is done with this, each teammate in the group can, in turn, repeat steps 3-7. Before doing so, make sure to pull the changes from teammates before starting to make new changes. (We’ll talk soon about ways to organize your work and resolve conflicts.)\n\n\nStart your first post\n\nTo start your first post first, run remotes::install_github(\"sussmanbu/quartopost\") in your Console.\n[[OP]] Run quartopost::quartopost() (or click Addins-&gt;Create Quarto Post, or use C-Shift-P, type “Create Quarto” and press enter to run the command).\n\nNow you can start working on your post. You’ll want to render your post to see what it will look like on the site.\n\nEvery time you want to make a new post, you can repeat step 2 above.\nWhen you want to publish your progress, follow steps 4-7 from Customize your site.\n\nFinally, make sure to read through everything on this site which has the directions and rubric for the final project."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA [46]15 Final Project",
    "section": "",
    "text": "Final Project due May 5, 2024 at 11:59pm.\nThis comes from the index.qmd file.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 31, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGeneral Tips\n\n\n\n\n\nSome small but important tips to follow. \n\n\n\n\n\nOct 4, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n\n\n\n\n\n\nGetting started\n\n\n\n\n\n\n\n\nDirections to set up your website and create your first post. \n\n\n\n\n\nFeb 23, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Team Meeting\n\n\n\n\n\n\n\n\nThis post details the steps you’ll take for your first team meeting. \n\n\n\n\n\nFeb 21, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "This comes from the file data.qmd.\nYour first steps in this project will be to find data to work on.\nI recommend trying to find data that interests you and that you are knowledgeable about. A bad example would be if you have no interest in video games but your data set is about video games. I also recommend finding data that is related to current events, social justice, and other areas that have an impact.\nInitially, you will study one dataset but later you will need to combine that data with another dataset. For this reason, I recommend finding data that has some date and/or location components. These types of data are conducive to interesting visualizations and analysis and you can also combine this data with other data that also has a date or location variable. Data from the census, weather data, economic data, are all relatively easy to combine with other data with time/location components."
  },
  {
    "objectID": "data.html#what-makes-a-good-data-set",
    "href": "data.html#what-makes-a-good-data-set",
    "title": "Data",
    "section": "What makes a good data set?",
    "text": "What makes a good data set?\n\nData you are interested in and care about.\nData where there are a lot of potential questions that you can explore.\nA data set that isn’t completely cleaned already.\nMultiple sources for data that you can combine.\nSome type of time and/or location component."
  },
  {
    "objectID": "data.html#where-to-keep-data",
    "href": "data.html#where-to-keep-data",
    "title": "Data",
    "section": "Where to keep data?",
    "text": "Where to keep data?\nBelow 50mb: In dataset folder\nAbove 50mb: In dataset-ignore folder which you will have to create manually. This folder will be ignored by git so you’ll have to manually sync these files across your team.\n\nSharing your data\nFor small datasets (&lt;50mb), you can use the dataset folder that is tracked by github. Stage and commit the files just like you would any other file.\nFor larger datasets, you’ll need to create a new folder in the project root directory named dataset-ignore. This will be ignored by git (based off the .gitignore file in the project root directory) which will help you avoid issues with Github’s size limits. Your team will have to manually make sure the data files in dataset-ignore are synced across team members.\nYour clean_data.R file in the scripts folder is the file where you will import the raw data that you download, clean it, and write .rds file(s) (using write_rds) that you’ll load in your analysis page. If desirable, you can have multiple scripts that produce different derived data sets, just make sure to link to them on this page.\nYou should never use absolute paths (eg. /Users/danielsussman/path/to/project/ or C:\\MA415\\\\Final_Project\\). Instead, use the here function from the here package to avoid path problems.\n\n\nClean data script\nThe idea behind this file is that someone coming to your website could largely replicate your analyses after running this script on the original data sets to clean them. This file might create a derivative data set that you then use for your subsequent analysis. Note that you don’t need to run this script from every post/page. Instead, you can load in the results of this script, which will usually be .rds files. In your data page you’ll describe how these results were created. If you have a very large data set, you might save smaller data sets that you can use for exploration purposes. To link to this file, you can use [cleaning script](/scripts/clean_data.R) wich appears as cleaning script."
  },
  {
    "objectID": "data.html#rubric-on-this-page",
    "href": "data.html#rubric-on-this-page",
    "title": "Data",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nDescribe where/how to find data.\n\nYou must include a link to the original data source(s). Make sure to provide attribution to those who collected the data.\nWhy was the data collected/curated? Who put it together? (This is important, if you don’t know why it was collected then that might not be a good dataset to look at.\n\nDescribe the different data files used and what each variable means.\n\nIf you have many variables then only describe the most relevant ones, possibly grouping together variables that are similar, and summarize the rest.\nUse figures or tables to help explain the data. For example, showing a histogram or bar chart for a particularly important variable can provide a quick overview of the values that variable tends to take.\n\nDescribe any cleaning you had to do for your data.\n\nYou must include a link to your clean_data.R file.\nRename variables and recode factors to make data more clear.\nAlso, describe any additional R packages you used outside of those covered in class.\nDescribe and show code for how you combined multiple data files and any cleaning that was necessary for that.\nSome repetition of what you do in your clean_data.R file is fine and encouraged if it helps explain what you did.\n\nOrganization, clarity, cleanliness of the page\n\nMake sure to remove excessive warnings, use clean easy-to-read code (without side scrolling), organize with sections, use bullets and other organization tools, etc.\nThis page should be self-contained."
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "This comes from the file analysis.qmd.\nWe describe here our detailed data analysis. This page will provide an overview of what questions you addressed, illustrations of relevant aspects of the data with tables and figures, and a statistical model that attempts to answer part of the question. You’ll also reflect on next steps and further analysis.\nThe audience for this page is someone like your class mates, so you can expect that they have some level of statistical and quantitative sophistication and understand ideas like linear and logistic regression, coefficients, confidence intervals, overfitting, etc.\nWhile the exact number of figures and tables will vary and depend on your analysis, you should target around 5 to 6. An overly long analysis could lead to losing points. If you want you can link back to your blog posts or create separate pages with more details.\nThe style of this paper should aim to be that of an academic paper. I don’t expect this to be of publication quality but you should keep that aim in mind. Avoid using “we” too frequently, for example “We also found that …”. Describe your methodology and your findings but don’t describe your whole process."
  },
  {
    "objectID": "analysis.html#note-on-attribution",
    "href": "analysis.html#note-on-attribution",
    "title": "Analysis",
    "section": "Note on Attribution",
    "text": "Note on Attribution\nIn general, you should try to provide links to relevant resources, especially those that helped you. You don’t have to link to every StackOverflow post you used but if there are explainers on aspects of the data or specific models that you found helpful, try to link to those. Also, try to link to other sources that might support (or refute) your analysis. These can just be regular hyperlinks. You don’t need a formal citation.\nIf you are directly quoting from a source, please make that clear. You can show long quotes using &gt; like this\n&gt; To be or not to be.\n\nTo be or not to be."
  },
  {
    "objectID": "analysis.html#rubric-on-this-page",
    "href": "analysis.html#rubric-on-this-page",
    "title": "Analysis",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nIntroduce what motivates your Data Analysis (DA)\n\nWhich variables and relationships are you most interested in?\nWhat questions are you interested in answering?\nProvide context for the rest of the page. This will include figures/tables that illustrate aspects of the data of your question.\n\nModeling and Inference\n\nThe page will include some kind of formal statistical model. This could be a linear regression, logistic regression, or another modeling framework.\nExplain the ideas and techniques you used to choose the predictors for your model. (Think about including interaction terms and other transformations of your variables.)\nDescribe the results of your modelling and make sure to give a sense of the uncertainty in your estimates and conclusions.\n\nExplain the flaws and limitations of your analysis\n\nAre there some assumptions that you needed to make that might not hold? Is there other data that would help to answer your questions?\n\nClarity Figures\n\nAre your figures/tables/results easy to read, informative, without problems like overplotting, hard-to-read labels, etc?\nEach figure should provide a key insight. Too many figures or other data summaries can detract from this. (While not a hard limit, around 5 total figures is probably a good target.)\nDefault lm output and plots are typically not acceptable.\n\nClarity of Explanations\n\nHow well do you explain each figure/result?\nDo you provide interpretations that suggest further analysis or explanations for observed phenomenon?\n\nOrganization and cleanliness.\n\nMake sure to remove excessive warnings, hide all code, organize with sections or multiple pages, use bullets, etc.\nThis page should be self-contained, i.e. provide a description of the relevant data."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This comes from the file about.qmd.\nThis is a website for the final project for MA[46]15 Data Science with R by Team 3. The members of this team are below."
  },
  {
    "objectID": "about.html#rodrigo-otero",
    "href": "about.html#rodrigo-otero",
    "title": "About",
    "section": "Rodrigo Otero",
    "text": "Rodrigo Otero\nRodrigo is an undergraduate Econ and Math senior. Github"
  },
  {
    "objectID": "about.html#yiqi-jin",
    "href": "about.html#yiqi-jin",
    "title": "About",
    "section": "Yiqi Jin",
    "text": "Yiqi Jin\nYiqi is an undergraduate Econ and Math Junior. Github"
  },
  {
    "objectID": "about.html#anshi-mittal",
    "href": "about.html#anshi-mittal",
    "title": "About",
    "section": "Anshi Mittal",
    "text": "Anshi Mittal\nAnshi is an undergraduate Data Science Junior"
  },
  {
    "objectID": "about.html#dan-sussman",
    "href": "about.html#dan-sussman",
    "title": "About",
    "section": "Dan Sussman",
    "text": "Dan Sussman\nDan is a professor in the Math/Stat department and is the instructor for the course. Github\n\n\nAbout this Template.\nThis is based off of the standard Quarto website template from RStudio (2023.09.0 Build 463)."
  },
  {
    "objectID": "big_picture.html",
    "href": "big_picture.html",
    "title": "Big Picture",
    "section": "",
    "text": "This comes from the file big_picture.qmd.\nThink of this page as your 538/Upshot style article. This means that you should try to tell a story through the data and your analysis. Read articles from those sites and similar sites to get a feeling for what they are like. Try to write in the style of a news or popular article. Importantly, this page should be geared towards the general public. You shouldn’t assume the reader understands how to interpret a linear regression or a complicated plot. Focus on interpretation and visualizations."
  },
  {
    "objectID": "big_picture.html#rubric-on-this-page",
    "href": "big_picture.html#rubric-on-this-page",
    "title": "Big Picture",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\n\nTitle\n\nYour big picture page should have a creative/click-bait-y title/headline that provides a hint about your thesis.\n\nClarity of Explanation\n\nYou should have a clear thesis/goal for this page. What are you trying to show? Make sure that you explain your analysis in detail but don’t go into top much mathematics or statistics. The audience for this page is the general public (to the extent possible). Your thesis should be a statement, not a question.\nEach figure should be very polished and also not too complicated. There should be a clear interpretation of the figure so the figure has a clear purpose. Even something like a histogram can be difficult to interpret for non-experts.\n\nCreativity\n\nDo your best to make things interesting. Think of a how a news article or a magazine story might draw you in. Think of how each part of your analysis supports the previous part or provides a different perspective.\n\nInteractive component\n\nQuality and ease of use of the interactive components. Is it clear what can be explored using your interactive components? Does it enhance and reinforce your conclusions?\n\nThis page should be self-contained.\n\nNote: This page should have no code visible, i.e. use #| echo: FALSE."
  },
  {
    "objectID": "big_picture.html#rubric-other-components",
    "href": "big_picture.html#rubric-other-components",
    "title": "Big Picture",
    "section": "Rubric: Other components",
    "text": "Rubric: Other components\n\nVideo Recording\nMake a video recording (probably using Zoom) demonstrating your interactive components. You should provide a quick explanation of your data and demonstrate some of the conclusions from your EDA. This video should be no longer than 4 minutes. Include a link to your video (and password if needed) in your README.md file on your Github repository. You are not required to provide a link on the website. This can be presented by any subset of the team members.\n\n\nRest of the Site\nFinally, here are important things to keep in mind for the rest of the site.\nThe main title of your page is informative. Each post has an author/description/informative title. All lab required posts are present. Each page (including the home page) has a nice featured image associated with it. Your about page is up to date and clean. You have removed the generic posts from the initial site template."
  },
  {
    "objectID": "Final Project.html",
    "href": "Final Project.html",
    "title": "Untitled",
    "section": "",
    "text": "Test\n\nTest"
  },
  {
    "objectID": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "href": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "title": "First Team Meeting",
    "section": "",
    "text": "These are the steps that you will take today to get started on your project. Today, you will just be brainstorming, and then next week, you’ll get started on the main aspects of the project.\n\nStart by introducing yourselves to each other. I also recommend creating a private channel on Microsoft Teams with all your team members. This will be a place that you can communicate and share ideas, code, problems, etc.\nDiscuss what aspects of the project each of you are more or less excited about. These include\n\nCollecting, cleaning, and munging data ,\nStatistical Modeling,\nVisualization,\nWriting about analyses, and\nManaging and reviewing team work.\n\nBased on this, discuss where you feel your strengths and weaknesses might be.\nNext, start brainstorming questions you hope to answer as part of this project. This question should in some way be addressing issues around racial disparities. The questions you come up with should be at the level of the question we started with when exploring the HMDA data. (“Are there differences in the ease of securing a loan based on the race of the applicant?”) You’ll revise your questions a lot over the course of the project. Come up with a few questions that your group might be interested in exploring.\nBased on these questions, start looking around for data that might help you analyze this. If you are looking at U.S. based data, data.gov is a good source and if you are looking internationally, I recommend checking out the World Bank. Also, try Googling for data. Include “data set” or “dataset” in your query. You might even include “CSV” or some other format. Using “data” by itself in your query often doesn’t work too well. Spend some time searching for data and try to come up with at least three possible data sets. (For your first blog post, you’ll write short proposals about each of them that I’ll give feedback on.)\nCome up with a team name. Next week, I’ll provide the Github Classroom assignment that will be where you work on your final project and you’ll have to have your team name finalized by then. Your project will be hosted online at the website with a URL like sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME.\n\nNext time, you’ll get your final project website set up and write your first blog post."
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html",
    "href": "posts/2023-12-20-examples/examples.html",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "href": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2025-03-03-blog-post-1/blog-post-1.html",
    "href": "posts/2025-03-03-blog-post-1/blog-post-1.html",
    "title": "Blog Post 1",
    "section": "",
    "text": "First Dataset:\nThe importance of distinguishing between urban and rural residential properties provides insight for analyzing ownership patterns, housing distribution, and land usage across different demographic groups.\nThis dataset can be used to examine the racial disparities in land ownership and how they may persist in terms of property size and urban-rural distribution. Historically, the US has confined minority groups to urban areas because of past segregation policies and limits their ability to acquire larger sets of land because of systemic racism. Since land and homeownership are major contributors to generational wealth, the dataset can also explore how land distribution contributes to the racial wealth gap by reflecting from broad economic inequities.\nThe IPUMS USA dataset accesses over 65,000 harmonized variables that cover a wide array of demographic, economic, social, and housing characteristics form 1790 to present data and compromises of billions of individual records and includes data from over 150 countries, but we will be focusing on Puerto Rico for research purposes.\nThe data will need to be manually selected and downloaded depending on the year and variables we want to focus on for analyzing trends. This can also serve as a challenge since we will need to have a clear understanding of the question we are trying to answer and be considerate about the variables we want to select.\nFinally, this data set will aim to answer questions such as the following: How does land ownership differ between urban and rural areas? Do racial disparities persist in property size and location? How does land distribution contribute to the racial wealth gap? What historical trends exist in land ownership in Puerto Rico?\nSource: IPUMS\n\nSecond Dataset:\n1.Data is from\nData-gov\n2.Give any details you can in regards to the number of rows and number of columns for your data.\nThe data consists of twelve columns and 6391 rows.\n3.Describe why and how the data was originally collected.\nIt describes Death rates for suicide, by sex, race, Hispanic origin, and age: United States. It satisfied racial and health topics. We get it because it has enough sample size, and with race and sex variables inside, we want to discover the relationships of suicide death rate between different races and between male and female.\n4.Are you able to load/clean the data?\nYes, The data variables do not have proper labels, and the variables for sex, race, and sex and race have been incorrectly combined (all placed in the same column). The data needs to be manually rearranged.\n5.What are the main questions you hope to address?\nWhich race has the highest suicide rate? Which gender has a higher suicide rate? After combining gender and race, which race has the highest suicide rate for a single gender? Which has the lowest? Investigate the underlying reasons.\n6.What challenges do you foresee?\nThe most difficult part is still processing the data. Splitting the combined “race and sex” into separate “race” and “sex” variables requires a significant amount of work.\n\nThird Dataset:\nExploring Racial Disparities Through Drug Overdose Data\nUnderstanding racial disparities in public health is essential for addressing systemic inequities. This dataset provides valuable insight into drug overdose death rates in the U.S., categorized by drug type, sex, age, race, and Hispanic origin. By analyzing this data, we can identify patterns in drug-related mortality and assess how different racial groups are affected. Drug Overdose Death Rates by Drug Type, Sex, Age, Race, and Hispanic Origin (United States) Source: Data.gov\nThis dataset, maintained by the Centers for Disease Control and Prevention (CDC), compiles data from national mortality records and public health reports. It tracks overdose deaths across different racial and ethnic groups and includes key demographic variables. This information is critical for examining racial disparities in drug-related deaths and identifying trends over time.\nDataset Details: Contains variables such as drug type, sex, age, race, and Hispanic origin. Tracks overdose rates over multiple years. Derived from official mortality and health data sources.\nResearch Questions: How do drug overdose death rates vary among racial and ethnic groups? Are specific drug types more prevalent in certain communities? How do age and gender intersect with racial disparities in overdose rates? Have overdose rates changed significantly for particular racial groups over time?\nPotential Challenges: Some racial groups may have underreported or incomplete data, which could impact analysis. The dataset may require extensive cleaning before conducting meaningful statistical evaluations. Socioeconomic and healthcare access factors are not included in the dataset but could provide additional context for disparities in overdose rates. This dataset provides an opportunity to explore racial disparities in drug-related deaths and identify underlying trends that contribute to these disparities. Understanding these patterns is essential for informing public health policies and improving targeted interventions to reduce overdose fatalities in disproportionately affected communities."
  },
  {
    "objectID": "posts/2025-03-24-blog-post-3/blog-post-3.html",
    "href": "posts/2025-03-24-blog-post-3/blog-post-3.html",
    "title": "Blog Post 3",
    "section": "",
    "text": "We began our project by loading a cleaned version of our housing dataset (housing_cleaned.rds) to enable efficient analysis. The original data was reviewed to ensure completeness and consistency, focusing on key variables like price and zipcode. We removed columns with substantial missing values and filtered out extreme outliers to ensure that summary statistics would be meaningful. The cleaned dataset was saved as dataset/cleaned_dataset.rds for consistent use throughout the project. To begin our exploratory data analysis, we examined the distribution of housing prices and found a heavily right-skewed pattern, indicating a concentration of homes under $1 million and a smaller number of high-end properties. This insight suggested that median price would be a better measure than the mean in summarizing the data.  Alongside technical preparation, we considered the ethical implications of our work using the Urban Institute’s Principles for Advancing Equitable Data Practice. One principle that stood out was the importance of transparency. Our dataset lacks demographic variables such as income, race, or household characteristics, which limits our ability to make conclusions about affordability or equity across different populations. Recognizing and communicating these limitations is critical to avoid misinterpretation or misuse of our findings. Another key principle was justice, particularly the responsibility to present results in ways that are accessible and useful to the communities most affected by housing trends. We plan to share simplified summaries and visualizations to ensure our insights are meaningful beyond academic settings. While our dataset is manageable and structured, the absence of demographic context poses a risk of overlooking structural inequities embedded in housing data. Differences in home prices across neighborhoods may reflect broader systemic issues like zoning, investment disparities, or historical segregation—factors not directly visible in the current dataset. Acknowledging this, we remain committed to ethical analysis and reporting, clearly outlining what our data can and cannot explain. Going forward, we aim to integrate additional context where possible and continue to align our methods with equity-centered practices."
  },
  {
    "objectID": "posts/2025-04-07-blog-post-5/blog-post-5.html",
    "href": "posts/2025-04-07-blog-post-5/blog-post-5.html",
    "title": "Blog Post 5",
    "section": "",
    "text": "We found a new data to compare with. https://www.kaggle.com/datasets/huyngohoang/housingcsv\nIncome Levels • Most addresses show annual incomes between $50,000 and $80,000, indicating that the sample group mainly consists of middle-class households. • There is noticeable regional variation in income, with coastal areas, military addresses (FPO/DPO), and locations near large cities tending to have higher median incomes. House Size • House sizes (estimated in square meters or square feet) typically fall between 5.0 and 8.5, with most values concentrated in the 6.0 to 7.5 range. • Larger homes tend to be found in the Midwest and rural areas (e.g., OK, PW, MO), possibly due to lower land costs. House Value vs. Income • In many cases, home values are roughly 20–30 times the annual income, aligning with typical U.S. housing market ratios. • For example, an income of $59,927.66 with a home value of $798,869.53 represents about a 13x multiple, suggesting affordable housing or strong purchasing power. • Conversely, in Taylorborough, OK, an income of $55,909.32 with a home value of $936,368.96 (about 17x) points to a higher housing cost burden. Population Density or Occupancy Metrics • Some locations like Lake Elizabeth, IN (6.17) and Janetbury, NM (8.09) show higher density, likely indicating urban or suburban environments. • In contrast, areas like Jacksonhaven, AZ (4.01) and Thomashaven, HI (6.62) show lower density, likely reflecting rural or less populated regions. Special Address Patterns • Multiple addresses start with FPO/DPO/USS/USNS, indicating military, diplomatic, or government-related residences. • These military-associated addresses tend to have income levels comparable to standard households, suggesting military families enjoy a middle-class lifestyle. Outliers • The lowest recorded income is $17,796.63 (Lake Martha, WY), significantly below the average—possibly representing retirees, low-income households, or geographically isolated areas. • The highest home value appears in Lake Kevin, UT at $1,762,214.68, which—given its associated income—likely reflects a luxury property or secondary real estate market.\nCombining datasets can present several challenges, particularly when dealing with housing data from different sources. In our case, integrating the original housing dataset from Kaggle with a new dataset introduced several difficulties: Inconsistent Data Formats: The two datasets utilized different formats for representing similar information. For instance, one dataset recorded dates in ‘MM/DD/YYYY’ format, while the other used ‘YYYY-MM-DD’. Additionally, categorical variables like property types were labeled differently across datasets (‘Single Family’ vs. ‘SFH’), necessitating meticulous standardization before integration. Disparate Feature Sets: Each dataset contained unique features not present in the other. The original dataset included detailed information on interior features such as the number of fireplaces and flooring types, whereas the new dataset provided data on neighborhood amenities and crime rates. This mismatch required careful consideration to align relevant features for a meaningful comparative analysis. Geographical Misalignment: The datasets covered overlapping but not identical geographical areas. The original dataset focused on urban regions, while the new dataset encompassed suburban areas. This discrepancy posed challenges in creating a unified dataset without introducing geographical biases. Temporal Differences: The datasets were collected during different time periods, with the original dataset covering 2015-2020 and the new dataset spanning 2018-2023. This temporal gap complicated trend analyses and required adjustments to account for time-based variations in the housing market. Addressing these challenges involved extensive data cleaning and transformation processes, including standardizing formats, reconciling feature differences, carefully selecting overlapping geographical areas, and aligning temporal aspects. Despite these efforts, some limitations persisted, underscoring the complexities inherent in combining disparate datasets for comprehensive analysis."
  },
  {
    "objectID": "posts/2025-04-22-blog-post-7/blog-post-7.html",
    "href": "posts/2025-04-22-blog-post-7/blog-post-7.html",
    "title": "Blog Post 5",
    "section": "",
    "text": "library(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.3\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'readr' was built under R version 4.3.3\n\n\nWarning: package 'purrr' was built under R version 4.3.3\n\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\nWarning: package 'stringr' was built under R version 4.3.3\n\n\nWarning: package 'forcats' was built under R version 4.3.3\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(shiny)\n\nWarning: package 'shiny' was built under R version 4.3.3\n\nlibrary(leaflet)\n\nWarning: package 'leaflet' was built under R version 4.3.3"
  },
  {
    "objectID": "posts/2025-04-22-blog-post-7/blog-post-7.html#race-based-income-distribution",
    "href": "posts/2025-04-22-blog-post-7/blog-post-7.html#race-based-income-distribution",
    "title": "Blog Post 5",
    "section": "Race-Based Income Distribution",
    "text": "Race-Based Income Distribution\n\nplotOutput(\"income_by_race\")\n\n\n\n\n\nrenderPlot({\n  housing_cleaned %&gt;%\n    ggplot(aes(x = factor(RACE), y = FTOTINC)) +\n    geom_boxplot(fill = \"skyblue\", outlier.shape = NA) +\n    scale_y_log10() +\n    labs(title = \"Household Income by Race (2023)\",\n         x = \"Race Code\",\n         y = \"Total Household Income (log scale)\")\n})"
  }
]